{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKLB52r2ElnoKoj5UTTKzF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install forex-python"],"metadata":{"id":"-YZaA2Inv9df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from forex_python.converter import CurrencyRates\n","c = CurrencyRates()\n","from tqdm import tqdm\n","import pandas as pd\n","from datetime import date, timedelta,datetime,date\n","import os\n","\n","\n","def get_week_number(date_obj):\n","  return (date_obj - date(date_obj.year,1,1)).days // 7 + 1\n","\n","def get_weekday_number(date_obj):\n","  return (date_obj - date(date_obj.year,1,1)).days % 7\n"],"metadata":{"id":"60DSASGBwN7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from genericpath import samestat\n","import pdb\n","from tqdm import tqdm\n","\n","def daterange(start_date, end_date):\n","    for n in range(int((end_date - start_date).days)):\n","        yield start_date + timedelta(n)\n","\n","def get_filename_from_YearAndWeek(YearAndWeek):\n","  year,week = YearAndWeek.split('_')\n","  return f'FOREX_Rate_{year}_week{week}.csv'\n","\n","\n","start_date = date(2000, 1, 1) #1st January 2000\n","end_date = date(2022, 12, 10) #10th December 2022\n","\n","save_dir = 'FOREX_Rate_by_week'\n","os.makedirs(save_dir,exist_ok=True)\n","\n","df_list = []\n","counter = 0\n","\n","for single_date in tqdm(daterange(start_date, end_date)):\n","    counter +=1\n","    if counter % 100 == 0:\n","      print(f\"{counter} days done\")\n","   \n","    day_data = c.get_rates('USD', single_date)\n","    day_data['Date'] = single_date.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","    day_data['YearAndWeek'] = f'{single_date.year}_{get_week_number(single_date)}'\n","    df_list.append(day_data)\n","\n","df = pd.DataFrame(df_list)\n","save_path = os.path.join(save_dir,get_filename_from_YearAndWeek(df_list[-1]['YearAndWeek']))\n","df.to_csv(save_path)\n","\n","      "],"metadata":{"id":"U0GqmLKZrE4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r FOREX_Rate_by_week.zip FOREX_Rate_by_week/"],"metadata":{"id":"fTTYCoGOpsZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!du -h FOREX_Rate_by_week.zip"],"metadata":{"id":"TrNIWz0FsI7u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp /content/FOREX_Rate_by_week.zip \"/content/drive/MyDrive/BDA Forex prediction project/.\""],"metadata":{"id":"doHiPcVwscGR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669909729973,"user_tz":300,"elapsed":189,"user":{"displayName":"Shailesh Sridhar","userId":"01152417468452907041"}},"outputId":"23911de6-b3b3-4dcb-bc3c-88277ed89457"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/meta_data.zip': No such file or directory\n"]}]},{"cell_type":"code","source":["import datetime\n","def timestamp_to_week(timestamp):\n","  date = timestamp[:10]\n","  year,month,day = [int(val) for val in date.split(\"-\")]\n","  week = datetime.date(year, month, day).isocalendar()[1]\n","  return str(week)\n","\n","def get_year_weeks_start_and_end(year):\n","  start_dates = []\n","  end_dates = []\n","\n","  first_day_of_year = date(year, 1, 1)\n","  week_start = first_day_of_year\n","  week_end = week_start+ timedelta(days=6)\n","\n","  while week_end.year == year:\n","    start_dates.append(week_start.strftime(\"%m%d\"))\n","    end_dates.append(week_end.strftime(\"%m%d\"))\n","\n","    week_start = week_end + timedelta(days=1)\n","    week_end = week_start+ timedelta(days=6)\n","  \n","  return start_dates,end_dates"],"metadata":{"id":"Hgxn3t7FuiP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import requests\n","import math\n","import csv\n","import sys\n","import time\n","from tqdm import tqdm\n","import os\n","import pdb\n","\n","# Data to be used while querying\n","url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?'\n","params = {\n","    \"api-key\": \"k7d8PIJozXNxppfbVMcmqKXh1j2is4BQ\",\n","    \"fq\": {\n","        \"source\": [\"The New York Times\"],\n","        \"section_name\": [\"Your Money\", \"Job Market\", \"Business\", \"World\", \"Business Day\", \"Technology\", \"Financial\", \"Politics\"],\n","        \"document_type\": [\"article\"]\n","    }\n","}\n","start_year = 20\n","end_year = 2023\n","api_keys = [\n","    \"k7d8PIJozXNxppfbVMcmqKXh1j2is4BQ\", \n","    \"srRafQT5F9h2QbSg2K7o5yvHyGGm6VXB\",\n","    \"ANAG6XCb39P2QjueyGhHh0DRo6YlgO3I\", \n","    \"qUBjJKERphRkfn26FXuShub0zNSUM5qI\", \n","    \"5IGEJJjATG4kG5jUR7ToPsNKqLNTK1xw\", \n","    \"G38sQ8GrIeOAcRWrWK0ioJ5Sd4fuA3mV\", \n","    \"W0J9rkchzD6YKgtAgm8qf8Unh42cgSWD\",\n","    \"0mXrz72SAxcLqdSubOZDMNTU40qflb6s\"  \n","]\n","\n","# Form the URL with the URL data\n","def generateURL(url, params):\n","    num_params = len(params.keys())\n","    for idx, param in enumerate(params):\n","        param_value = \"\"\n","        if param != 'fq':\n","            param_value = params[param]\n","        else:\n","            num_filters = len(params[param].keys())\n","            for index, filterField in enumerate(params[param]):\n","                filter_values = \" \".join(['\"{}\"'.format(x) for x in params[param][filterField]])\n","                param_value += \"{}:({})\".format(filterField, filter_values)\n","                if index < num_filters-1:\n","                    param_value += ' AND '\n","        url += \"{}={}\".format(param, param_value)\n","        if idx < num_params-1:\n","            url += \"&\"\n","    return url\n","\n","os.makedirs('meta_data',exist_ok=True)\n","\n","for k in range(end_year - start_year):\n","    # For each year, find articles for each quarter\n","    week_start,week_end= get_year_weeks_start_and_end(start_year+k)\n","    for week in range(len(week_start)):\n","       \n","        begin_date = \"{}{}\".format(start_year+k, week_start[week])\n","        end_date = \"{}{}\".format(start_year+k, week_end[week])\n","\n","        queryURL = generateURL(url,params)\n","        queryURL += \"&begin_date={}&end_date={}\".format(begin_date, end_date)\n","\n","        # Loop through all pages (each page has data of 10 articles)\n","        api_swapper = 1\n","        current_api_key = api_keys[api_swapper]\n","\n","        # Fet the data first to get the total number of hits\n","        print(\"{} - {}\".format(start_year+k, week+1))\n","        try:\n","            response = (requests.get(queryURL)).json()\n","            # If some issue with response, switch the API key\n","            while True:\n","                if 'response' not in response:\n","                    api_swapper = (api_swapper + 1) % len(api_keys)\n","                    queryURL = queryURL.replace(current_api_key, api_keys[api_swapper])\n","                    current_api_key = api_keys[api_swapper]\n","                    time.sleep(0.8)\n","                    response = (requests.get(queryURL)).json()\n","                else:\n","                    break\n","            pages = min(1, math.ceil(response['response']['meta']['hits']/10))\n","            pbar = tqdm(total=pages) # Only to show the progress bar\n","\n","            articles_meta_data = []\n","            file_count = 1\n","            \n","            # Keep track of duplicate articles\n","            duplicate_articles = []\n","            duplicate_articles_url = []\n","            \n","            with open('meta_data/articles_{}_week{}.csv'.format(start_year+k, week+1), 'w') as output_file:\n","                for i in range(pages):\n","                    current_url = queryURL + '&page={}'.format(i)\n","                    response = (requests.get(current_url)).json()\n","                    # If some issue with response, switch the API key\n","                    while True:\n","                        if 'response' not in response:\n","                            api_swapper = (api_swapper + 1) % len(api_keys)\n","                            queryURL = queryURL.replace(current_api_key, api_keys[api_swapper])\n","                            current_api_key = api_keys[api_swapper]\n","                            time.sleep(0.8)\n","                            response = (requests.get(queryURL)).json()\n","                        else:\n","                            break\n","                    articles = response['response']['docs']\n","                    for article in articles:\n","                        articles_meta_data.append({\n","                            \"_id\": article[\"_id\"],\n","                            \"url\": article[\"web_url\"], # URL of the article\n","                            \"word_count\": article[\"word_count\"], # Number of words in the article\n","                            \"section\": article[\"section_name\"], # News paper section under which the article is printed \n","                            \"date\": article[\"pub_date\"], # Article published date\n","                            \"type\": article[\"news_desk\"], # Type of topic (business, sports, economic etc)\n","                            \"headline\": article[\"headline\"][\"main\"].replace('\\n', ' '), # Main headline of the article\n","                            \"abstract\": article[\"abstract\"].replace('\\n', ' ')\n","                        })\n","                    \n","                    # Update the progress bar\n","                    time.sleep(0.1)\n","                    pbar.update(1)\n","\n","                    # Keep swapping API keys\n","                    api_swapper = (api_swapper + 1) % len(api_keys)\n","                    queryURL = queryURL.replace(current_api_key, api_keys[api_swapper])\n","                    current_api_key = api_keys[api_swapper]\n","                    time.sleep(0.8)\n","                keys = articles_meta_data[0].keys()\n","                writer = csv.DictWriter(output_file, keys)\n","                writer.writeheader()\n","                writer.writerows(articles_meta_data)\n","                pbar.close()\n","        except:\n","            print(\"Something went wrong while fetching the data! \\n\")\n","            print(sys.exc_info())\n","            print(response)\n","            raise"],"metadata":{"id":"Y2Bgr35C1ufE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import multiprocessing as mp\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import csv\n","import time\n","from tqdm import tqdm\n","import os\n","import os.path\n","\n","# Total range of years\n","start_year = 2000\n","end_year = 2023\n","\n","# Customise which year, quarter and row to start from and which year to end at\n","start_from_year = 2000\n","start_from_quarter = 1\n","start_from_row = 1\n","stop_at_year = 2023\n","\n","\n","def worker(row, q):\n","    row_text = []\n","    row['article'] = row['headline'] + row['abstract']\n","    row_text = [row['_id'],row['url'],row['word_count'],row['section'],row['date'],row['type'],row['headline'].replace('\\n', ''),row['abstract'].replace('\\n', ''),row['article'].replace('\\n', '')]\n","    q.put(row_text)\n","    return row_text\n","\n","def listener(q):\n","    while 1:\n","        m = q.get()\n","        if m == 'kill':\n","            break\n","\n","for year in range(end_year - start_year):\n","    # Stop at the mentioned year\n","    if start_year+year == stop_at_year:\n","        break\n","    \n","    # Skip fetching previously covered data\n","    if (start_year+year < start_from_year):\n","        continue\n","    \n","    out_file_name = 'articles/articles_{}.csv'.format(start_year+year)\n","    columns = ['_id','url','word_count','section','date','type','headline','abstract','article']\n","    with open(out_file_name, 'a+') as out_file:\n","        writer = csv.writer(out_file)\n","        \n","        # If file is created new or empty, write column names first\n","        if os.path.isfile(out_file_name):\n","            if (os.stat(out_file_name).st_size == 0):\n","                writer.writerow(columns)\n","        else:\n","            writer.writerow(columns)\n","\n","        print(\"processing year:\",start_year+year)\n","        for week in tqdm(range(53)):\n","            \n","            # Skip Quarters as per the customisations\n","            if (start_year+year == start_from_year) and (week < start_from_quarter-1):\n","                continue\n","            if not os.path.exists('meta_data/articles_{}_week{}.csv'.format(start_year+year, week+1)):\n","                continue\n","            with open('meta_data/articles_{}_week{}.csv'.format(start_year+year, week+1)) as meta_file:\n","                data = csv.DictReader(meta_file)\n","                num_articles = 2000\n","                manager = mp.Manager()\n","                q = manager.Queue()    \n","                pool = mp.Pool(10)\n","                \n","                #put listener to work first\n","                watcher = pool.apply_async(listener, (q,))\n","                \n","                #fire off workers\n","                jobs = []\n","                for row in data:\n","                    job = pool.apply_async(worker, (row, q))\n","                    jobs.append(job)\n","                \n","                # collect results from the workers through the pool result queue\n","                for job in jobs:\n","                    row_val = job.get()\n","                    if not row_val:\n","                        continue\n","                    writer.writerow(row_val)\n","                    \n","                #now we are done, kill the listener\n","                q.put('kill')\n","                pool.close()\n","                pool.join()"],"metadata":{"id":"kGWqFKDj2FTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.makedirs(\"/content/articles_by_week\",exist_ok=True)\n","\n","def parse_to_weeks(articles_folder,weeks_folder):\n","  for csv_file in os.listdir(articles_folder):\n","    csv_path = os.path.join(articles_folder,csv_file)\n","    df = pd.read_csv(csv_path)\n","\n","    df['week'] = df['date'].apply(timestamp_to_week)\n","\n","    unique_weeks = df.week.unique()\n","\n","    for week in unique_weeks:\n","      week_path = os.path.join(weeks_folder,f\"{csv_file.split('.')[0]}_week{week}.csv\")\n","      df[df['week'] == week].to_csv(week_path)\n","\n","\n","parse_to_weeks(\"/content/articles\",\"/content/articles_by_week\")"],"metadata":{"id":"quZWuNswPk2x"},"execution_count":null,"outputs":[]}]}